A=apply(S,1,check)
table(A)
nrow(S)
rep("b",3)
315/1820
check = function(x) sum(x=="r")==2 & sum(x=="y")==1 & sum(x=="b")==1
A=apply(S,1,check)
table(A)
378/1820
S
S[S$X1=="r" & S$X2=="r",]
S1=S[S$X1=="r" & S$X2=="r",]
S2=S1[S1$X3=="r & S1$X4=="r",]
S2=S1[S1$X3=="r" & S1$X4=="r",]
nrow(S2)
35/1820
X = -exp(-x-y)
X = -exp(-x)
X = -exp(^-x)
records = read.csv("./data/FlightRecords.csv")
records=records[records$deptime>600 & records$deptime < 2300, ]
records$deptime=as.factor(floor(records$deptime/100))
records$dayweek = as.factor(records$dayweek)
records$weather = as.factor(records$weather)
records$delay = factor(records$delay, level=c("ontime","delayed"), labels=c("ontime","delayed"))
records=records[-c(1,5,6,7,11,12)]
str(records)
auc1 = performance(pred1, measure = "auc")
# 사용할 패키지 추가
library(ggplot2)
library(psych)
library(caret)
library(rsample)
library(glmnet)
library(ROCR)
records = read.csv("./data/FlightRecords.csv")
records=records[records$deptime>600 & records$deptime < 2300, ]
records$deptime=as.factor(floor(records$deptime/100))
records$dayweek = as.factor(records$dayweek)
records$weather = as.factor(records$weather)
records$delay = factor(records$delay, level=c("ontime","delayed"), labels=c("ontime","delayed"))
records=records[-c(1,5,6,7,11,12)]
set.seed(123)
split = initial_split(records, prop=0.7, strat="delay")
train = training(split)
test = testing(split)
pred_base = factor(test$weather, levels=c(0,1), labels=c("ontime","delayed"))
model = glm(delay~., data=train, family="binomial")
test_prob = predict(model, test, type="response")
test_pred = rep("ontime",651)
test_pred[test_prob > 0.2] = "delayed"
model_step = step(model, direction="backward")
coef(model_step)
prob_step = predict(model_step, test, type="response")
pred_step = rep("ontime",651)
pred_step[prob_step>0.5] = "delayed"
confusionMatrix(factor(pred_step),test$delay, positive = "delayed")
trainx = model.matrix(delay~., data=train)
trainy = train$delay
set.seed(123)
cv_lasso = cv.glmnet(x=trainx, y = trainy, alpha=1, family="binomial",type.measure = "auc", nfolds = 10)
lambda  = cv_lasso$lambda[19]
coef(cv_lasso, s = lambda)
pred_prob = predict(cv_lasso, newx=model.matrix(delay~.,data=test), s= lambda, type="response")
pred_class = predict(cv_lasso, newx=model.matrix(delay~.,data=test), s= lambda, type="class")
test_prob = predict(model, test, type="response")
prob_step = predict(model_step, test, type="response")
pred_prob = predict(cv_lasso, newx=model.matrix(delay~.,data=test), s= lambda, type="response")
pred1 = prediction(test_prob, test$delay, c("ontime","delayed"))
perf1 = performance(pred1, measure="tpr",x.measure = "fpr")
pred2 = prediction(prob_step, test$delay, c("ontime","delayed"))
perf2 = performance(pred2, measure = "tpr", x.measure = "fpr")
pred3 = prediction(pred_prob, test$delay, c("ontime","delayed"))
perf3= performance(pred3, measure = "tpr", x.measure = "fpr")
plot(perf1, col="darkred", lwd=3)
plot(perf2, col="darkblue", lwd=3, add=TRUE)
plot(perf3, col="darkgreen", lwd=3, add=TRUE)
auc1 = performance(pred1, measure = "auc")
auc2 = performance(pred2, measure = "auc")
auc3 = performance(pred3, measure = "auc")
auc1@y.values
auc2@y.values
auc3@y.values
auc1@y.values
auc2@y.values
auc3@y.values
str(train)
str(test)
train_labels = train[,7]
str(train_labels)
test_labels=test[,7]
z_normalized = c("center","scale")
cv <- trainControl(method="repeatedcv", number = 5, repeats = 5)
tune_grid <- expand.grid(k = seq(1, 99, 2))
knn_fit <- train(data=train, delay~., method="knn", trControl = cv,
preProcess = z_normalized, tuneGrid = tune_grid)
knn_fit
ggplot(knn_fit)+theme_bw()
knn_fit$bestTune
test_pred <- predict(knn_fit, test[,-1])
str(test)
test_pred <- predict(knn_fit, test[,-1])
test_pred <- predict(knn_fit, test)
test_pred
confusionMatrix(test_pred, test_labels, positive="delayed")
confusionMatrix(test_pred, test_labels, positive="ontime")
confusionMatrix(test_pred, test_labels, positive="delayed")
library(ISLR)
data = OJ
str(data)
set.seed(123)
tune.out = tune(svm, Purchase~., data=data, kernel = "linear", ranges=list(cost=10^seq(-3,3)))
library(e1071)
tune.out = tune(svm, Purchase~., data=data, kernel = "linear", ranges=list(cost=10^seq(-3,3)))
summary(tune.out)
bestmodel = tune.out$best.model
bestmodel
set.seed(123)
split = initial_split(data, prop=0.7, strat="Purchase")
train = training(split)
test = testing(split)
set.seed(123)
tune.out = tune(svm, Purchase~., data=train, kernel = "linear", ranges=list(cost=10^seq(-3,3)))
summary(tune.out)
summary(tune.out)
set.seed(123)
split = initial_split(data, prop=0.7, strat="Purchase")
train = training(split)
test = testing(split)
set.seed(123)
tune.out = tune(svm, Purchase~., data=train, kernel = "linear", ranges=list(cost=10^seq(-3,3)))
summary(tune.out)
bestmodel = tune.out$best.model
pred = predict(bestmodel, test)
sample(c(-1,1), 20, rep=TRUE)
test$Purchase
confusionMatrix(pred, test$Purchase)
set.seed(123)
split = initial_split(data, prop=0.7, strat="Purchase")
train = training(split)
test = testing(split)
tune.out = tune(svm, Purchase~., data=train, kernel = "linear", ranges=list(cost=10^seq(-3,3)))
summary(tune.out)
set.seed(123)
split = initial_split(data, prop=0.7, strat="Purchase")
train = training(split)
test = testing(split)
set.seed(123)
tune.out = tune(svm, Purchase~., data=train, kernel = "linear", ranges=list(cost=10^seq(-3,3)))
summary(tune.out)
#backward selection
set.seed(123)
model_step = step(model, direction="backward")
#상관계수
coef(model_step)
#확률
prob_step = predict(model_step, test, type="response")
#threshold값 을 통한 test데이터 예측
pred_step = rep("ontime",651)
pred_step[prob_step>0.5] = "delayed"
confusionMatrix(factor(pred_step),test$delay, positive = "delayed")
#RBF kernel
set.seed(123)
tune.out = tune(svm, Purchase~.,data=train, kernel="radial", ranges=list(cost=c(0.01,0.1,1,10,100,1000), gamma=c(0.01,0.1,1,10,100)))
summary(tune.out)
bestmodel = tune.out$best.model
pred = predict(bestmodel, test)
confusionMatrix(pred, test$Purchase)
set.seed(123)
tune.out = tune(svm, Purchase~., data=train, kernel = "linear", ranges=list(cost=10^seq(-3,3)))
summary(tune.out)
bestmodel = tune.out$best.model
pred = predict(bestmodel, test)
confusionMatrix(pred, test$Purchase)
#RBF kernel
set.seed(123)
tune.out1 = tune(svm, Purchase~.,data=train, kernel="radial", ranges=list(cost=c(0.01,0.1,1,10,100,1000), gamma=c(0.01,0.1,1,10,100)))
bestmodel1 = tune.out$best.model
pred1 = predict(bestmodel1, test)
confusionMatrix(pred1, test$Purchase)
bestmodel1 = tune.out1$best.model
pred1 = predict(bestmodel1, test)
confusionMatrix(pred1, test$Purchase)
tune.out2 = tune(svm, Purchase~.,data=train, kernel="polynomial", ranges=list(cost=c(0.1,1,10,100,1000), degree=c(2,3,4)))
summary(tune.out2)
summary(tune.out)
bestmodel2 = tune.out2$best.model
pred2 = predict(bestmodel2, test)
confusionMatrix(pred2, test$Purchase)
library(ISLR)
library(rsample)
library(rpart)
library(rpart.plot)
install.packages(rpart.plot)
library(rpart.plot)
install.packages(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
library(caret)
Hitters = na.omit(Hitters)
split = initial_split(Hitters, prop=0.7, strata = "Salary")
Hitters_train = training(split)
Hitters_test = testing(split)
set.seed(123)
rt = rpart(Salary~., data=Hitters_train, method="anova",control=list(cp=0))
rpart.plot(rt)
rt = rpart(Salary~., data=Hitters_train, method="anova",control=list(cp=0,maxdepth=3))
rpart.plot(rt)
rt = rpart(Salary~., data=Hitters_train, method="anova",control=list(cp=0))
rpart.plot(rt)
printcp(rt)
printcp(rt)
plotcp(rt)
install.packages("randomforest")
install.packages("randomForest")
library(randomForest)
library(vip)
set.seed(123)
split = initial_split(Hitters, prop=0.7, strata="Salary")
Hitters_train = training(split)
Hitters_test = testing(split)
set.seed(123)
bag = randomForest(Salary~., data=Hitters_train, ntree=300, mtry=19)
bag
bag$predicted
Hitters
plot(bag)
bag$oob.times
bag$importance
vip(bag)
x=0:50
n=20
N=150
dhyper(x,12,138,n)
df=dhyper(x,12,138,n)
library(Rstat)
disc.exp(x,df)
df=dhyper(3,12,138,n)
df
1-sum(dhyper(0:2,12,138,n))
dgeom(x-1,p)
dgeom(30,0.035)
dbinom(96,4,0.035)
x=0:350
r=4
dbinom(x-r,r,0.035)
2.5*5.5
sqrt(4)
sqrt(2)/sqrt(6)
3*sqrt(2)/sqrt(6)
3*sqrt(2)/sqrt(6) + 13.75
dnbinom(x-r,r,0.035)
df=dnbinom(x-r,r,0.035)
disc.exp(x,df)
dnbinom(96,4,0.035)
x=0:250; r=4
df = dnbinom(x-r,r,0.035)
disc.exp(x,df)
x=4:1000; r=4
df = dnbinom(x-r,r,0.035)
disc.exp(x,df)
x=4:10000; r=4
df = dnbinom(x-r,r,0.035)
disc.exp(x,df)
dnbinom(96,4,0.035)
sqrt(6)+sqrt(3)
1-sum(dnbinom(x-3,3,0.035))
x=0:100
sum(dnbion(x-r,r,0.035))
sum(dnbinom(x-r,r,0.035))
x=4:100
sum(dnbinom(x-r,r,0.035))
r
sqrt(3)+13.75
?dnbinom
?cov
dgeom(30,0.035)
sqrt(6)+sqrt(3)
#데이터 읽기
mnist <- dslabs::read_mnist()
train_x=mnist$train$images[0:2000,]
train_y=mnist$train$labels[0:2000]
library(ggplot2)
ggplot(train_y)
ggplot(train_y, aes(x=.))
train_yG = as.data.frame(train_y)
ggplot(train_yG, aes(x=.))
str(train_yG)
ggplot(train_yG, aes(train_y))+geom_bar()
ggplot(train_yG, aes(train_y))+geom_bar(fill="cornflowerblue",color="black")+labs(x="target data", y="Frequency", title="target data distribution")
ggplot(train_yG, aes(train_y))+geom_bar(fill="cornflowerblue",color="black")+labs(x="target data", y="Frequency", title="target data distribution")+geom_text(aes(label=n))
ggplot(train_yG, aes(train_y))+geom_bar(fill="cornflowerblue",color="black")+labs(x="target data", y="Frequency", title="target data distribution")+geom_text(aes(label=train_y))
ggplot(train_yG, aes(train_y))+geom_bar(fill="cornflowerblue",color="black")+labs(x="target data", y="Frequency", title="target data distribution")+geom_text(aes(label=train_yG$train_y))
ggplot(train_yG, aes(train_y))+geom_bar(fill="cornflowerblue",color="black")+labs(x="target data", y="Frequency", title="target data distribution")+geom_text(aes(label=train_yG$train_y),vjust=-0.5)
ggplot(train_yG, aes(train_y))+geom_bar(fill="cornflowerblue",color="black")+labs(x="target data", y="Frequency", title="target data distribution")+geom_text(aes(label=train_y),vjust=-0.5)
ggplot(train_yG, aes(x=train_y,y=n))+geom_bar(fill="cornflowerblue",color="black")+labs(x="target data", y="Frequency", title="target data distribution")+geom_text(aes(label=n),vjust=-0.5)
ggplot(train_yG, aes(x=train_y))+geom_bar(fill="cornflowerblue",color="black")+labs(x="target data", y="Frequency", title="target data distribution")+geom_text(vjust=-0.5)
ggplot(train_yG, aes(x=train_y))+geom_bar(fill="cornflowerblue",color="black")+labs(x="target data", y="Frequency", title="target data distribution")
ggplot(train_yG, aes(x=train_y))+geom_bar(fill="cornflowerblue",color="black")+labs(x="target data", y="Frequency", title="target data distribution")+scale_x_continuous(breaks=seq(0,9,1))
train_x.colnames
colnames(train_x)
str(train_x)
train_x = as.data.frame(train_x)
str(train_x)
train_x = as.matrix(train_x)
str(train_x)
train_x
train_x=mnist$train$images[0:2000,]
train_x = as.data.frame(train_x)
str(train_x)
train_x=mnist$train$images[0:2000,]
colnames(train_x)=c(rep(1,784,1))
rep(1,784)
str(rep(1,784))
colnames(train_x)=c(rep(1,784))
train_x
str(train_x)
colnames(train_x)=c(paste0("v",rep(1,784)))
train_x=mnist$train$images[0:2000,]
train_x=mnist$train$images[0:2000,]
str(train_x)
colnames(train_x)=c(paste0("v",rep(1,784)))
str(train_x)
train_x
rep(1,784)
rep(1:784)
colnames(train_x)=c(paste0("v",rep(1:784)))
str(train_x)
train_x = as.data.frame(train_x)
str(train_x)
train_x=mnist$train$images[0:2000,]
colnames(train_x)=c(paste0("v",rep(1:784)))
str(train_x)
train_x
nearZeroVar(train_x)
library(caret)
nearZeroVar(train_x)
o_index=nearZeroVar(train_x)
train_x[,o_index]
str(train_x[,o_index])
train_x[,-c(o_index)]
x=train_x[,-c(o_index)]
str(x)
o_index
str(o_index)
train_x=train_x[,-c(o_index)]
str(train_x)
str(train_x)
train = as.data.frame(train_x,train_y)
str(train)
train_y
?as.data.frame
train =data.frame(train_x,train_y)
str(train)
train$train_y
train_y
train_x
str(train_x)
str(train)
str(train$v154)
str(train$v155)
train_x=mnist$train$images[0:2000,]
train_x
str(train$v157)
train_x=mnist$train$images[0:2000,]
train_y=mnist$train$labels[0:2000]
colnames(train_x)=c(paste0("v",rep(1:784)))
o_index=nearZeroVar(train_x)
train_x=train_x[,-c(o_index)]
str(train_x)
train =data.frame(train_x,train_y)
str(train)
test_x = mnist$test$images[0:2000,]
test_y=mnist$test$labels[0:2000]
str(test_x)
str(test_y)
colnames(test_x)=c(paste0("v",rep(1:784)))
test_x=test_x[,-c(o_index)]
str(test_x)
str(train_x)
colnames(test_x)
colnames(test_y)
colnames(train_x)
colnames(train_x) = colnames(test_x)
train_x=mnist$train$images[0:2000,]
train_y=mnist$train$labels[0:2000]
test_x = mnist$test$images[0:2000,]
test_y=mnist$test$labels[0:2000]
colnames(train_x)=c(paste0("v",rep(1:784)))
o_index=nearZeroVar(train_x)
train_x=train_x[,-c(o_index)]
#출발 공항 별 연착비율
train =data.frame(train_x,train_y)
colnames(test_x)=c(paste0("v",rep(1:784)))
test_x=test_x[,-c(o_index)]
colnames(train_x) == colnames(test_x)
test =data.frame(test_x,test_y)
str(test)
test_x = mnist$test$images
str(test_x)
test_x = mnist$test$images
test_y=mnist$test$labels
colnames(test_x)=c(paste0("v",rep(1:784)))
test_x=test_x[,-c(o_index)]
test =data.frame(test_x,test_y)
str(test)
str(train)
colnames(test_x)==colnames(train_x)
image(1:28, 1:28, matrix(mnist$test$images[1,], nrow=28)[ , 28:1], col =
gray(seq(0, 1, 0.05)), xlab = "", ylab="")
# 사용할 패키지 추가
library(dslabs)
library(ggplot2)
library(caret)
image(1:28, 1:28, matrix(mnist$test$images[1,], nrow=28)[ , 28:1], col =
gray(seq(0, 1, 0.05)), xlab = "", ylab="")
#데이터 읽기
mnist <- dslabs::read_mnist()
image(1:28, 1:28, matrix(mnist$test$images[1,], nrow=28)[ , 28:1], col =
gray(seq(0, 1, 0.05)), xlab = "", ylab="")
sdfsdf
mnist
mnist <- dslabs::read_mnist()
library(dslabs)
mnist = dslabs::read_mnist()
library(Rstat)
ch7.man()
fx = function(x) dunif(x,0,1)
fy = function(y) dunif(y,2,6)
win.graph(7,6)
th = 10; lam = 1/th
pexp(5+3, lam, low=F)
pexp(5+3, lam, low=T)
cont.mpdf
cont.mpdf("exp",0,3,para=1/10, ymax=5)
pexp(5+3, lam, low=F)
cont.mpdf("exp",0,10,para=1/10, ymax=5)
exp(-0.3)
alp = 2
the=5
pgamma(3, alp, 1/the, lower=F)
dgamma(1,alp,1/the)
dgamma(2,alp,1/the)
pgamma(3, alp, 1/the, lower=T)
dgamma(0,alp,1/the)
dgamma(1,alp,1/the)
dgamma(2,alp,1/the)
dgamma(3,alp,1/the)
dgamma(3,alp,rate = 1/the)
dgamma(1,alp,1/the) + dgamma(2,alp,1/the)
dgamma(1,alp,1/the) + dgamma(2,alp,1/the)+dgamma(3,alp,1/the)
dgamma(3,alp,rate = 1/the)
dgamma(1:3,alp,rate = 1/the)
pgamma(2, alp, 1/the, lower=T)
pgamma(2, alp, 1/the, lower=F)
pgamma(1, alp, 1/the, lower=T)
dgamma(1,alp,rate = 1/the)
cont.mpdf("gamma",0,8,para=alp, para2=the, ymax=1.2)
library(Rstat)
cont.mpdf("gamma",0,8,para=alp, para2=the, ymax=1.2)
dgamma(2,alp,the)
dgamma(4,alp,the)
dgamma(4,alp,scale=the)
pgamma(4,alp,1/the,lower=T)
pgamma(8,alp,1/the,lower=T)
library(Rstat)
x=0:20
fx=dhyper(x,50,950,30)
disc.exp(x,fx,prt=TRUE)
x=0:50
fx=dhyper(x,50,950,30)
disc.exp(x,fx,prt=TRUE)
disc.eek
log(0.9)
pgamma(3,alp,1/the,lower=F)
alp = 4
the=5
pgamma(15,alp,1/the,lower=F)
pgamma(25,alp,1/the,lower=F)
pgamma(25,alp,1/the,lower=F)/pgamma(15,alp,1/the,lower=F)
pgamma
2. 지수분포
-f(x)=Lam*e^(-Lam*x), x>0
-E(x)=1/Lam , var(x)=1/Lam^2
-P(X>x)=e^(-Lam*x)
exp(-3/4)
e(1)
exp(e)
exp(1)
5. 베타분포(shape1=alp, shape2=the, ncp=비중심모수)
-pbeta(x,shape1,shape2,ncp=0,lower=TRUE)
?pbeta
pbeta(70,5,2,ncp=0,lower=F)
pbeta(0.07,5,2,ncp=0,lower=F)
pbeta(0.7,5,2,ncp=0,lower=F)
pbeta(0.8,3,1,ncp=0)-pbeta(0.7,3,1,ncp=0)
exp(-15/2)
exp(-1/2)
alp=4
the=5
choose(12,10)*pgamma(15,alp,1/the,lower=F)
exp(-15/2)
5/6*pgamma(15,alp,1/the,lower=F)+11/12*pgamma(15,alp.1/the,lower=F)+pgamma(15,alp,1/the,lower=F)
5/6*pgamma(15,alp,1/the,lower=F)+11/12*pgamma(15,alp.1/the,lower=F)+pgamma(15,alp,1/the,lower=F)
5/6*pgamma(15,alp,1/the,lower=F)+11/12*pgamma(15,alp,1/the,lower=F)+pgamma(15,alp,1/the,lower=F)
2.7*0.47236
setwd("C:/Users/seook/2021CodingStudy/21-1데이터분석및활용(R)")
data = read.csv("./data/Tweets_win.csv", encoding = "UTF-8")
data
str(data)
data$airline
setwd("C:/Users/seook/2021CodingStudy/21-1데이터분석및활용(R)")
